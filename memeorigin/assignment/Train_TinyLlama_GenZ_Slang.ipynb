{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune TinyLlama for Gen Z Slang Explanation - V2 CORRECTED\n",
    "\n",
    "**Assignment**: Implementing Autoregressive Models (ARM)\n",
    "\n",
    "**Model**: TinyLlama-1.1B-Chat-v1.0\n",
    "\n",
    "**Technique**: LoRA (Low-Rank Adaptation) Fine-tuning\n",
    "\n",
    "**Dataset**: 1,779 Gen Z slang terms\n",
    "\n",
    "---\n",
    "\n",
    "## V2 Changes:\n",
    "- Removed template placeholders from training data\n",
    "- Simplified format to match inference exactly\n",
    "- Model will learn actual content, not templates!\n",
    "\n",
    "## Steps:\n",
    "1. Install dependencies\n",
    "2. Upload training data (V2!)\n",
    "3. Load and prepare data (new format)\n",
    "4. Load TinyLlama base model\n",
    "5. Configure LoRA\n",
    "6. Train (3 epochs, ~30-60 min)\n",
    "7. Test and save adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q transformers peft datasets accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload Training Data V2\n",
    "\n",
    "**IMPORTANT**: Upload `genz_slang_training_v2.jsonl` to Colab:\n",
    "- Click the folder icon on the left\n",
    "- Click the upload button\n",
    "- Select `genz_slang_training_v2.jsonl` (NOT the old v1 file!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the V2 file is uploaded\n",
    "import os\n",
    "if os.path.exists('genz_slang_training_v2.jsonl'):\n",
    "    print(\"[OK] Training data V2 found!\")\n",
    "    with open('genz_slang_training_v2.jsonl', 'r') as f:\n",
    "        num_examples = len(f.readlines())\n",
    "    print(f\"  {num_examples} training examples loaded\")\n",
    "else:\n",
    "    print(\"[ERROR] Training data not found!\")\n",
    "    print(\"Please upload genz_slang_training_v2.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and Prepare Data (V2 Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load JSONL data - V2 already has proper text format\n",
    "data = []\n",
    "with open('genz_slang_training_v2.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(data)} examples\")\n",
    "\n",
    "# Show first example to verify format\n",
    "print(\"\\nFirst training example:\")\n",
    "print(\"=\" * 70)\n",
    "print(data[0]['text'])\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create HuggingFace dataset\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# Split into train/validation (90/10)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(f\"\\nTraining examples: {len(dataset['train'])}\")\n",
    "print(f\"Validation examples: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load TinyLlama Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Model name\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"[OK] Model loaded successfully!\")\n",
    "print(f\"  Model size: {model.num_parameters() / 1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                      # Rank of LoRA matrices\n",
    "    lora_alpha=32,             # Scaling factor\n",
    "    target_modules=[           # Which modules to apply LoRA to\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,         # Dropout for regularization\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"      # Causal Language Modeling (Autoregressive)\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "print(\"\\n[OK] LoRA configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tinyllama-lora-genz-slang-v2\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    warmup_steps=50,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train the Model\n",
    "\n",
    "**This will take 30-60 minutes on Colab free GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "# Tokenize all data\n",
    "print(\"Tokenizing training data...\")\n",
    "tokenized_train = dataset['train'].map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_test = dataset['test'].map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Causal LM, not masked LM\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"This will take 30-60 minutes.\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train!\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[OK] Training complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test the Model (IMPORTANT!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with the V2 format (matches inference exactly!)\n",
    "def test_model(term):\n",
    "    # This matches inference.py exactly\n",
    "    prompt = f\"\"\"Task: Explain the internet slang.\n",
    "Term: {term}\n",
    "\n",
    "Definition:\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=80,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        repetition_penalty=1.2,\n",
    "    )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the generated part\n",
    "    generated = result[len(prompt):].strip()\n",
    "    return result, generated\n",
    "\n",
    "# Test on slang terms\n",
    "test_terms = [\"w\", \"dank\", \"rizz\", \"fr\", \"bet\"]\n",
    "\n",
    "print(\"Testing the fine-tuned model:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for term in test_terms:\n",
    "    print(f\"\\nTERM: {term}\")\n",
    "    print(\"‚îÄ\"*70)\n",
    "    full, generated = test_model(term)\n",
    "    print(\"Generated:\")\n",
    "    print(generated[:200])  # First 200 chars\n",
    "    print(\"‚îÄ\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save the LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Create output directory with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "output_dir = f\"tinyllama-lora@{timestamp}\"\n",
    "\n",
    "# Save only the LoRA adapter (not the full model)\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"[OK] LoRA adapters saved to: {output_dir}\")\n",
    "print(\"\\nContents:\")\n",
    "!ls -lh {output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Download the Adapters\n",
    "\n",
    "**IMPORTANT**: Download these files to your computer:\n",
    "\n",
    "1. Right-click on the folder icon (left sidebar)\n",
    "2. Navigate to `tinyllama-lora@YYYY-MM-DD/`\n",
    "3. Download these files:\n",
    "   - `adapter_config.json`\n",
    "   - `adapter_model.safetensors`\n",
    "\n",
    "Then replace the files in your local project:\n",
    "```\n",
    "memeorigin/services/slang-explainer/models/adapters/tinyllama-lora@YYYY-MM-DD/\n",
    "```\n",
    "\n",
    "And update `inference.py` line 6 with the new date!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Zip the adapter folder for easier download\n",
    "import shutil\n",
    "\n",
    "shutil.make_archive(output_dir, 'zip', output_dir)\n",
    "print(f\"[OK] Created {output_dir}.zip\")\n",
    "print(\"\\nDownload this file and extract it to your project!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary - V2 Improvements\n",
    "\n",
    "‚úÖ Fixed training format (no more template placeholders!)\n",
    "\n",
    "‚úÖ Trained TinyLlama-1.1B on 1,779 Gen Z slang terms\n",
    "\n",
    "‚úÖ Used LoRA for efficient fine-tuning\n",
    "\n",
    "‚úÖ Format matches inference exactly\n",
    "\n",
    "### What Changed:\n",
    "\n",
    "**OLD (V1):**\n",
    "```\n",
    "Task: Explain...\n",
    "Term: w\n",
    "Format:\n",
    "Definition: <one or two sentences>  ‚Üê Model copied this\n",
    "Example: <one sentence>              ‚Üê And this\n",
    "```\n",
    "\n",
    "**NEW (V2):**\n",
    "```\n",
    "Task: Explain the internet slang.\n",
    "Term: w\n",
    "\n",
    "Definition: Shorthand for win        ‚Üê Actual content!\n",
    "Example: Got the job today, big W!   ‚Üê Actual content!\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "1. Download the adapter files\n",
    "2. Replace in your project\n",
    "3. Update `inference.py` line 6\n",
    "4. Test with `demo_with_fallback.py`\n",
    "5. Model should generate REAL definitions now!\n",
    "\n",
    "**Good luck! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
